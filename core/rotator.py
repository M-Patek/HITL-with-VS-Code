import google.generativeai as genai
from google.api_core import exceptions
from google.generativeai import caching # [Phase 1 Upgrade] Import Caching
import time
import logging
import threading
import datetime
from typing import List, Dict, Any, Tuple, Optional

logger = logging.getLogger("GeminiRotator")

# [Concurrency Fix] å…¨å±€é”ï¼Œé˜²æ­¢å¤šçº¿ç¨‹ä¸‹ genai.configure äº’ç›¸è¦†ç›–
_GENAI_GLOBAL_LOCK = threading.Lock()

class AllKeysExhaustedError(Exception):
    """Raised when all available API keys have been tried and failed."""
    pass

class GeminiKeyRotator:
    def __init__(self, base_url: str, keys: List[str]):
        if not keys:
            raise ValueError("API Key list cannot be empty.")
            
        self.keys = keys
        self.current_index = 0
        self.base_url = base_url
        # è¿™æ˜¯ç”¨äºè½®è¯¢ Key ç´¢å¼•çš„é”
        self._index_lock = threading.Lock()
        
        # [Phase 1 Upgrade] ç®€å•çš„ç¼“å­˜å…ƒæ•°æ®è®°å½• (Key -> CacheName)
        # æ³¨æ„ï¼šContext Caching æ˜¯ç»‘å®šåˆ° Project/Key çš„ï¼Œè½®è¯¢ Key å¯èƒ½ä¼šå¯¼è‡´ç¼“å­˜å¤±æ•ˆæˆ–æ— æ³•è®¿é—®ã€‚
        # ç­–ç•¥ï¼šå¦‚æœå¯ç”¨äº†ç¼“å­˜ï¼Œæš‚æ—¶é”å®šä½¿ç”¨å½“å‰çš„ Keyã€‚
        self.active_cache_name = None
        self.cached_key_index = -1

    def _get_next_key(self):
        with self._index_lock:
            # å¦‚æœæœ‰æ´»è·ƒçš„ç¼“å­˜ï¼Œä¸”æˆ‘ä»¬è¿˜åœ¨é‡è¯•èŒƒå›´å†…ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨åˆ›å»ºäº†ç¼“å­˜çš„é‚£ä¸ª Key
            if self.active_cache_name and self.cached_key_index != -1:
                return self.keys[self.cached_key_index]

            key = self.keys[self.current_index]
            self.current_index = (self.current_index + 1) % len(self.keys)
        return key

    def create_context_cache(self, model_name: str, content: str, ttl_minutes: int = 10) -> Optional[str]:
        """
        [Phase 1 Upgrade] åˆ›å»º Gemini Context Cache
        """
        # è·å–å½“å‰æŒ‡å‘çš„ Key
        key = self.keys[self.current_index]
        
        with _GENAI_GLOBAL_LOCK:
            try:
                genai.configure(api_key=key)
                
                # åˆ›å»ºç¼“å­˜
                # æ³¨æ„ï¼šContext Caching æœ‰æœ€å° token é™åˆ¶ (é€šå¸¸ 32k+)ï¼Œå¤ªçŸ­çš„å†…å®¹å»ºç¼“å­˜åè€Œæ…¢
                # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œå‡è®¾å†…å®¹å·²ç»è¶³å¤Ÿé•¿ã€‚å®é™…ä½¿ç”¨ä¸­å¯ä»¥åŠ é•¿åº¦åˆ¤æ–­ã€‚
                if len(content) < 1000: # ç¨å¾®æ”¾å®½é™åˆ¶ä»¥ä¾¿æµ‹è¯•
                    logger.info("Content too short for caching, skipping.")
                    return None

                # ä½¿ç”¨å½“å‰æ—¶é—´æˆ³é˜²æ­¢é‡åå†²çª
                unique_suffix = int(time.time())
                cache = caching.CachedContent.create(
                    model=model_name,
                    display_name=f"gemini_swarm_repo_map_{unique_suffix}",
                    system_instruction=content,
                    ttl=datetime.timedelta(minutes=ttl_minutes),
                )
                
                self.active_cache_name = cache.name
                self.cached_key_index = self.current_index
                logger.info(f"ğŸ’¾ Context Cache created: {cache.name} (Key Index: {self.current_index})")
                return cache.name
                
            except Exception as e:
                logger.warning(f"Failed to create context cache: {e}")
                return None

    def call_gemini_with_rotation(
        self, 
        model_name: str, 
        contents: List[Dict[str, Any]], 
        system_instruction: str = None,
        cached_content_name: str = None, # [Phase 1 Upgrade] ä¼ å…¥ç¼“å­˜åç§°
        complexity: str = "simple",
        max_retries: int = None # If None, defaults to len(keys)
    ) -> Tuple[str, Dict[str, int]]:
        """
        è°ƒç”¨ Gemini API å¹¶è‡ªåŠ¨è½®è¯¢ Keyã€‚
        [Concurrency Fix] ä½¿ç”¨å…¨å±€é”ä¿æŠ¤é…ç½®å’Œç”Ÿæˆè¿‡ç¨‹ã€‚
        [Phase 1 Upgrade] æ”¯æŒ Cached Contentã€‚
        """
        if max_retries is None:
            max_retries = len(self.keys) * 2 # Allow 2 cycles
            
        retries = 0
        last_error = None
        
        while retries < max_retries:
            # å¦‚æœæŒ‡å®šäº†ç¼“å­˜ï¼Œå¼ºè¡Œä½¿ç”¨ç»‘å®šäº†ç¼“å­˜çš„é‚£ä¸ª Keyï¼Œä¸è½®è¯¢
            if cached_content_name and self.cached_key_index != -1:
                key = self.keys[self.cached_key_index]
            else:
                key = self._get_next_key()

            try:
                # [Concurrency Fix] è¿™æ˜¯ä¸€ä¸ªä¸´ç•ŒåŒºã€‚
                with _GENAI_GLOBAL_LOCK:
                    genai.configure(api_key=key)
                    
                    # [Phase 1 Upgrade] å¤„ç†ç¼“å­˜é€»è¾‘
                    model = None
                    if cached_content_name:
                         # å¿…é¡»é€šè¿‡ get è·å–ç¼“å­˜å¯¹è±¡
                        try:
                            # æ³¨æ„ï¼šCachedContent.get() å¯èƒ½ä¸ç›´æ¥è¿”å›å¯ç”¨äº GenerativeModel çš„å¯¹è±¡
                            # ä½† SDK é€šå¸¸å…è®¸é€šè¿‡ from_cached_content åŠ è½½
                            cache_obj = caching.CachedContent.get(cached_content_name)
                            model = genai.GenerativeModel.from_cached_content(cached_content=cache_obj)
                            logger.info(f"âš¡ Using Cached Context: {cached_content_name}")
                        except Exception as cache_err:
                            logger.warning(f"Cache lookup failed: {cache_err}, falling back to regular")
                            # é™çº§ï¼šå¦‚æœæ‰¾ä¸åˆ°ç¼“å­˜ï¼Œä½¿ç”¨æ™®é€šç³»ç»Ÿæç¤º
                            model = genai.GenerativeModel(
                                model_name=model_name,
                                system_instruction=system_instruction
                            )
                    else:
                        model = genai.GenerativeModel(
                            model_name=model_name,
                            system_instruction=system_instruction
                        )
                    
                    generation_config = genai.types.GenerationConfig(
                        temperature=0.2 if complexity == "complex" else 0.1,
                        max_output_tokens=8192
                    )

                    response = model.generate_content(
                        contents,
                        generation_config=generation_config
                    )
                    
                    # Usage extraction
                    usage = {
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0
                    }
                    
                    if hasattr(response, 'usage_metadata'):
                        usage["prompt_tokens"] = response.usage_metadata.prompt_token_count
                        usage["completion_tokens"] = response.usage_metadata.candidates_token_count
                        usage["total_tokens"] = response.usage_metadata.total_token_count
                    
                    return response.text, usage

            except exceptions.ResourceExhausted:
                # å¦‚æœæ˜¯ç¼“å­˜æ¨¡å¼ä¸”é…é¢è€—å°½ï¼Œè¿™æ˜¯ä¸€æ¡æ­»èƒ¡åŒï¼Œå¿…é¡»æ”¾å¼ƒç¼“å­˜ï¼Œåˆ‡æ¢ Key é‡è¯•
                if cached_content_name:
                    logger.warning(f"Key for Cache {cached_content_name} exhausted. Abandoning cache.")
                    cached_content_name = None # é™çº§ä¸ºæ— ç¼“å­˜æ¨¡å¼
                    self.active_cache_name = None
                
                logger.warning(f"Key {key[:8]}... exhausted. Rotating.")
                retries += 1
                time.sleep(1)
                last_error = "Quota Exceeded"
            except Exception as e:
                logger.error(f"Gemini API Error with key {key[:8]}...: {e}")
                retries += 1
                last_error = str(e)
                if cached_content_name:
                     cached_content_name = None # å‡ºé”™ä¹Ÿé™çº§
                time.sleep(2 * min(retries, 5)) # Exponential backoff capped
        
        # If we reach here, all retries failed
        logger.error("All API Keys exhausted or failed.")
        raise AllKeysExhaustedError(f"Failed after {retries} retries. Last error: {last_error}")
