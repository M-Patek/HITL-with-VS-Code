import requests
from bs4 import BeautifulSoup
import logging
import re

logger = logging.getLogger("Tools-Browser")

class WebLoader:
    """
    [Continue Soul] ç½‘é¡µå†…å®¹æŠ“å–å·¥å…· (@Docs)
    ç”¨äºå®æ—¶æŠ“å–åœ¨çº¿æ–‡æ¡£ï¼Œæ‰©å…… AI çš„çŸ¥è¯†åº“ã€‚
    """
    def __init__(self):
        # ä¼ªè£…æˆæµè§ˆå™¨ï¼Œé˜²æ­¢è¢«ç®€å•çš„åçˆ¬æ‹¦æˆª
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

    def scrape_url(self, url: str) -> str:
        """æŠ“å– URL å¹¶è½¬æ¢ä¸ºç®€åŒ–æ–‡æœ¬"""
        try:
            logger.info(f"ğŸŒ Scraping: {url}")
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. ç§»é™¤æ— å…³å…ƒç´  (å™ªéŸ³æ¸…æ´—)
            for element in soup(["script", "style", "nav", "footer", "iframe", "svg", "noscript"]):
                element.decompose()
            
            # 2. æå–æ ‡é¢˜
            title = soup.title.string if soup.title else url
            
            # 3. æå–ä¸»è¦æ–‡æœ¬
            # get_text ä½¿ç”¨æ¢è¡Œç¬¦åˆ†éš”å—çº§å…ƒç´ 
            text = soup.get_text(separator='\n')
            
            # 4. æ¸…ç†ç©ºè¡Œå’Œå¤šä½™ç©ºæ ¼
            lines = (line.strip() for line in text.splitlines())
            # å°†å¤šè¡Œæ–‡æœ¬åˆå¹¶ï¼Œä¿ç•™æ®µè½ç»“æ„
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            clean_text = '\n'.join(chunk for chunk in chunks if chunk)
            
            # 5. æˆªæ–­ä»¥é˜²è¿‡é•¿ (Gemini Context Window å¾ˆå¤§ï¼Œä½†è¿˜æ˜¯èŠ‚çº¦ç‚¹ Token)
            max_length = 20000 
            if len(clean_text) > max_length:
                clean_text = clean_text[:max_length] + "\n\n...[Content Truncated]..."

            return f"### ğŸŒ Source: {title}\nURL: {url}\n\n{clean_text}"
            
        except Exception as e:
            logger.error(f"Scrape failed: {e}")
            return f"[Error] Could not scrape {url}: {str(e)}"
