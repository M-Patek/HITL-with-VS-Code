import requests
from bs4 import BeautifulSoup
import logging
import socket
from urllib.parse import urlparse
import ipaddress

logger = logging.getLogger("Tools-Browser")

class WebLoader:
    """
    [Continue Soul] ç½‘é¡µå†…å®¹æŠ“å–å·¥å…· (@Docs)
    ç”¨äºå®æ—¶æŠ“å–åœ¨çº¿æ–‡æ¡£ï¼Œæ‰©å…… AI çš„çŸ¥è¯†åº“ã€‚
    """
    def __init__(self):
        # ä¼ªè£…æˆæµè§ˆå™¨
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

    def _is_safe_url(self, url: str) -> bool:
        """
        [Security Fix] SSRF é˜²å¾¡æ£€æµ‹
        æ£€æŸ¥è§£æåçš„ IP æ˜¯å¦ä¸ºç§æœ‰åœ°å€æˆ–ç¯å›åœ°å€ã€‚
        """
        try:
            parsed = urlparse(url)
            hostname = parsed.hostname
            if not hostname:
                return False

            # è§£æ IP
            try:
                ip = socket.gethostbyname(hostname)
            except socket.gaierror:
                return False # æ— æ³•è§£æçš„åŸŸåè§†ä¸ºä¸å®‰å…¨æˆ–ä¸å¯è¾¾

            ip_obj = ipaddress.ip_address(ip)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç§æœ‰ã€ç¯å›ã€é“¾è·¯æœ¬åœ°ç­‰ä¿ç•™åœ°å€
            if (ip_obj.is_private or 
                ip_obj.is_loopback or 
                ip_obj.is_link_local or 
                ip_obj.is_reserved):
                logger.warning(f"ğŸš« Blocked SSRF attempt to {hostname} ({ip})")
                return False
                
            # ä»…å…è®¸ http å’Œ https
            if parsed.scheme not in ('http', 'https'):
                return False

            return True

        except Exception as e:
            logger.error(f"URL validation error: {e}")
            return False

    def scrape_url(self, url: str) -> str:
        """æŠ“å– URL å¹¶è½¬æ¢ä¸ºç®€åŒ–æ–‡æœ¬"""
        
        # 1. å®‰å…¨æ£€æŸ¥
        if not self._is_safe_url(url):
            return f"[Security Blocked] Access to {url} is denied due to SSRF protection."

        try:
            logger.info(f"ğŸŒ Scraping: {url}")
            # è®¾ç½®åˆç†çš„ timeout
            response = requests.get(url, headers=self.headers, timeout=5)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 2. ç§»é™¤æ— å…³å…ƒç´  (å™ªéŸ³æ¸…æ´—)
            for element in soup(["script", "style", "nav", "footer", "iframe", "svg", "noscript"]):
                element.decompose()
            
            # 3. æå–æ ‡é¢˜
            title = soup.title.string if soup.title else url
            
            # 4. æå–ä¸»è¦æ–‡æœ¬
            text = soup.get_text(separator='\n')
            
            # 5. æ¸…ç†ç©ºè¡Œå’Œå¤šä½™ç©ºæ ¼
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            clean_text = '\n'.join(chunk for chunk in chunks if chunk)
            
            # 6. æˆªæ–­ä»¥é˜²è¿‡é•¿
            max_length = 20000 
            if len(clean_text) > max_length:
                clean_text = clean_text[:max_length] + "\n\n...[Content Truncated]..."

            return f"### ğŸŒ Source: {title}\nURL: {url}\n\n{clean_text}"
            
        except Exception as e:
            logger.error(f"Scrape failed: {e}")
            return f"[Error] Could not scrape {url}: {str(e)}"
